{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path initialisation\n",
    "\n",
    "BASE_DIR='./'\n",
    "TEXT_DATA_DIR=BASE_DIR + 'Data/'\n",
    "#TEXT_DATA_FILE='train.csv'\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "Header=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def load_data(file_name,_type):\n",
    "    \"function to read data from directory\"\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    with open(os.path.join(TEXT_DATA_DIR,file_name),\"r\",encoding='utf-8') as f:\n",
    "        if Header:\n",
    "            _=next(f)\n",
    "        if _type=='train':\n",
    "            for line in f:\n",
    "                _,temp_y, temp_x =line.rstrip('\\n').split(',',2)\n",
    "                x.append(temp_x.replace(\"'\", \"\"))\n",
    "                y.append(temp_y)\n",
    "            return x,y\n",
    "        else:\n",
    "            for line in f:\n",
    "                _, temp_x =line.rstrip('\\n').split(',',1)\n",
    "                x.append(temp_x.replace(\"'\", \"\"))\n",
    "            return x\n",
    "            \n",
    "x, y = load_data('train.csv','train')\n",
    "y = np.asarray(y, dtype='int8')\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(x, np.asarray(y, dtype='int32'),test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=y)\n",
    "test=load_data('test.csv','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: @user   bihday greg t\n",
      "Sentence in indexes:\n",
      "  [1, 61, 9833, 651]\n",
      "Sentence fitted to max length:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    1   61 9833  651]\n"
     ]
    }
   ],
   "source": [
    "# Initialise dictionary size and maximum sentence length\n",
    "\n",
    "MAX_NB_WORDS=10000\n",
    "MAX_SEQUENCE_LENGTH=40\n",
    "\n",
    "print('Original sentence:',data_train[0])\n",
    "\n",
    "# Create a dictionary with Tokenizer\n",
    "tokenizer=Tokenizer(num_words=MAX_NB_WORDS, filters='#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!\"')\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "# Replace words with there indexes from out dictionary\n",
    "X_train= tokenizer.texts_to_sequences(data_train)\n",
    "X_val=tokenizer.texts_to_sequences(data_val)\n",
    "\n",
    "print(\"Sentence in indexes:\\n \", X_train[0])\n",
    "\n",
    "\n",
    "# fit each sentence to max length\n",
    "X_train=pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val=pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Sentence fitted to max length:\\n\", X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3759\n",
      "11\n",
      "3458\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts['is'])\n",
    "print(tokenizer.word_index['is'])\n",
    "print(tokenizer.word_docs['is'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with vector representation: 400000\n"
     ]
    }
   ],
   "source": [
    "# Path to embedding file\n",
    "EMBEDDINGS_DIR= BASE_DIR + 'Embeddings'\n",
    "ZIP_FILE='glove.6B.zip'\n",
    "EMBEDDINGS_FILE='glove.6B.50d.txt'\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Choosing only first 10000 words\n",
    "first ={k:v for k, v in tokenizer.word_index.items() if v<10000}\n",
    "\n",
    "# Upload embeddings\n",
    "embeddings={}\n",
    "with zipfile.ZipFile(os.path.join(EMBEDDINGS_DIR,ZIP_FILE)) as myzip:\n",
    "    with myzip.open(EMBEDDINGS_FILE) as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0].decode('UTF-8')\n",
    "            coefs=np.asarray(values[1:],dtype='float32')\n",
    "            embeddings[word]=coefs\n",
    "            \n",
    "            del values,word,coefs,line\n",
    "            \n",
    "print(\"Number of words with vector representation:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "\n",
    "embeddings_matrix=np.zeros((tokenizer.num_words,EMBEDDING_DIM))\n",
    "for word, i in first.items():\n",
    "    embedding_vector=embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "Name=\"SimpleRNN\"\n",
    "\n",
    "#Embedding layer initailisation\n",
    "\n",
    "embedding_layer= Embedding(tokenizer.num_words, EMBEDDING_DIM, weights=[embeddings_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "model= Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28765 samples, validate on 3197 samples\n",
      "Epoch 1/10\n",
      "28765/28765 [==============================] - 25s - loss: 0.1988 - acc: 0.9326 - val_loss: 0.1848 - val_acc: 0.9302\n",
      "Epoch 2/10\n",
      "28765/28765 [==============================] - 25s - loss: 0.1673 - acc: 0.9409 - val_loss: 0.1966 - val_acc: 0.9327\n",
      "Epoch 3/10\n",
      "28765/28765 [==============================] - 25s - loss: 0.1612 - acc: 0.9422 - val_loss: 0.1946 - val_acc: 0.9159\n",
      "Epoch 4/10\n",
      "28765/28765 [==============================] - 22s - loss: 0.1472 - acc: 0.9482 - val_loss: 0.1707 - val_acc: 0.9415\n",
      "Epoch 5/10\n",
      "28765/28765 [==============================] - 23s - loss: 0.1448 - acc: 0.9487 - val_loss: 0.1608 - val_acc: 0.9440\n",
      "Epoch 6/10\n",
      "28765/28765 [==============================] - 22s - loss: 0.1354 - acc: 0.9517 - val_loss: 0.1705 - val_acc: 0.9453\n",
      "Epoch 7/10\n",
      "28765/28765 [==============================] - 19s - loss: 0.1227 - acc: 0.9562 - val_loss: 0.1546 - val_acc: 0.9468\n",
      "Epoch 8/10\n",
      "28765/28765 [==============================] - 20s - loss: 0.1155 - acc: 0.9604 - val_loss: 0.1561 - val_acc: 0.9440\n",
      "Epoch 9/10\n",
      "28765/28765 [==============================] - 20s - loss: 0.1051 - acc: 0.9627 - val_loss: 0.1556 - val_acc: 0.9478\n",
      "Epoch 10/10\n",
      "28765/28765 [==============================] - 22s - loss: 0.1028 - acc: 0.9650 - val_loss: 0.1592 - val_acc: 0.9462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4cd5c3550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, labels_train, validation_data=[X_val, labels_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
